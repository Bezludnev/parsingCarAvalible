# app/services/scraper_service.py - –ò–°–ü–†–ê–í–õ–ï–ù: –¥–æ–±–∞–≤–ª–µ–Ω—ã –∏–º–ø–æ—Ä—Ç—ã –∏ –º–µ—Ç–æ–¥ –¥–ª—è –æ–¥–Ω–æ–π –º–∞—à–∏–Ω—ã
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import re
import asyncio
from datetime import datetime
from typing import List, Dict, Optional, Set, Any
from app.config import settings
from app.schemas.car import CarCreate
import httpx
import logging

logger = logging.getLogger(__name__)


class ScraperService:
    def __init__(self):
        self.options = Options()
        self.options.add_argument('--headless')
        self.options.add_argument('--disable-gpu')
        self.options.add_argument('--no-sandbox')
        self.options.add_argument('--disable-dev-shm-usage')

    def _fetch_description(self, link: str) -> Optional[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ"""
        try:
            with httpx.Client(timeout=10.0) as client:
                resp = client.get(link)
                if resp.status_code != 200:
                    return None
                soup = BeautifulSoup(resp.text, 'html.parser')
                desc_div = soup.find('div', class_='js-description')
                if desc_div:
                    paragraphs = [p.get_text(' ', strip=True) for p in desc_div.find_all('p')]
                    return ' '.join(paragraphs)
        except Exception:
            return None
        return None

    def _create_driver(self) -> webdriver.Chrome:
        """Create Chrome driver using path from settings"""
        service = Service(executable_path=settings.chromedriver_path)
        self.options.add_argument(
            '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        self.options.add_argument('--accept-language=en-US,en;q=0.9')
        self.options.add_argument('--disable-blink-features=AutomationControlled')
        return webdriver.Chrome(service=service, options=self.options)

    def _has_urgent_keywords(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ urgent –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        if not text:
            return False

        urgent_keywords = [
            '—Å—Ä–æ—á–Ω–æ', 'urgent', '–±—ã—Å—Ç—Ä–æ', 'asap', 'must sell', 'price drop',
            'reduced', 'negotiable', 'open to offers', '—Ç–æ—Ä–≥', '–æ–±–º–µ–Ω',
            '–≤—ã–≥–æ–¥–Ω–æ', '–Ω–µ–¥–æ—Ä–æ–≥–æ', '–¥–µ—à–µ–≤–æ', '—Å–Ω–∏–∂–µ–Ω–∞ —Ü–µ–Ω–∞', '—É–º–µ—Å—Ç–µ–Ω —Ç–æ—Ä–≥'
        ]

        text_lower = text.lower()
        return any(keyword in text_lower for keyword in urgent_keywords)

    def _should_skip_ad(self, ad, existing_links: Set[str]) -> tuple[bool, str]:
        """üéØ –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏–µ"""
        title_tag = ad.find("a", class_="advert__content-title")
        if not title_tag:
            return True, "no_title_tag"

        link = "https://www.bazaraki.com" + title_tag.get('href', '')

        if link in existing_links:
            return True, "already_exists"

        return False, ""

    def _parse_car_data(self, ad, filter_config: Dict, existing_links: Set[str]) -> Optional[CarCreate]:
        """–ü–∞—Ä—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ –∞–≤—Ç–æ–º–æ–±–∏–ª—è —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è"""

        # –°–Ω–∞—á–∞–ª–∞ –±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –µ—Å—Ç—å –ª–∏ —É–∂–µ –≤ –±–∞–∑–µ
        should_skip, reason = self._should_skip_ad(ad, existing_links)
        if should_skip:
            if reason == "already_exists":
                logger.debug("‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ")
            return None

        # Title –∏ link (—É–∂–µ –ø—Ä–æ–≤–µ—Ä–∏–ª–∏ –≤ _should_skip_ad)
        title_tag = ad.find("a", class_="advert__content-title")
        title = title_tag.text.strip()
        link = "https://www.bazaraki.com" + title_tag.get('href', '')

        # Price
        price_tag = ad.find("a", class_="advert__content-price")
        price = price_tag.text.strip().replace("\n", " ") if price_tag else "–Ω–µ—Ç —Ü–µ–Ω—ã"

        # Parse features
        features_tag = ad.find("div", class_="advert__content-features")
        features = []
        mileage = None
        year = None

        if features_tag:
            for f in features_tag.find_all("div", class_="advert__content-feature"):
                text = f.text.strip()
                features.append(text)

                # Parse mileage
                mileage_match = re.search(r'([\d,. ]+)\s*km', text.lower())
                if mileage_match:
                    mileage_str = mileage_match.group(1).replace(',', '').replace(' ', '')
                    try:
                        mileage = int(mileage_str)
                    except:
                        pass

                # Parse year
                year_match = re.search(r'(19\d{2}|20\d{2})', text)
                if year_match:
                    try:
                        year = int(year_match.group(1))
                    except:
                        pass

        # Parse year from title if not found
        if year is None:
            title_year_match = re.search(r'(19\d{2}|20\d{2})', title)
            if title_year_match:
                try:
                    year = int(title_year_match.group(1))
                except:
                    pass

        # Date and place
        hint_tag = ad.find("div", class_="advert__content-hint")
        date_posted, place = "–Ω–µ—Ç –¥–∞—Ç—ã", "–Ω–µ—Ç –≥–æ—Ä–æ–¥–∞"

        if hint_tag:
            date_tag = hint_tag.find("div", class_="advert__content-date")
            if date_tag:
                date_posted = date_tag.text.strip()

            place_tag = hint_tag.find("div", class_="advert__content-place")
            if place_tag:
                place = place_tag.text.strip()

        # Description (–∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ - —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π!)
        desc_tag = ad.find("div", class_="advert__description")
        if not desc_tag:
            desc_tag = ad.find("div", class_="advert__content-description")
        description = desc_tag.text.strip() if desc_tag else None
        if not description:
            description = self._fetch_description(link)

        # üî• URGENT MODE LOGIC - –±–æ–ª–µ–µ –º—è–≥–∫–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã
        is_urgent_mode = filter_config.get("urgent_mode", False)

        if is_urgent_mode:
            # –í urgent —Ä–µ–∂–∏–º–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            has_urgent_text = self._has_urgent_keywords(title) or self._has_urgent_keywords(description)

            # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–ª—è urgent
            if year and year < filter_config.get("min_year", 0):
                if year < (filter_config.get("min_year", 0) - 2):
                    return None

            if mileage and mileage > filter_config.get("max_mileage", float('inf')):
                max_allowed = filter_config.get("max_mileage", float('inf'))
                if has_urgent_text:
                    max_allowed += 50000  # –ë–æ–Ω—É—Å –¥–ª—è urgent –æ–±—ä—è–≤–ª–µ–Ω–∏–π
                if mileage > max_allowed:
                    return None

            logger.info(f"üî• Urgent —Ä–µ–∂–∏–º: {filter_config.get('filter_name')} - –Ω–∞–π–¥–µ–Ω–∞ –º–∞—à–∏–Ω–∞"
                        f" {'(URGENT keywords!)' if has_urgent_text else ''}")
        else:
            # –û–±—ã—á–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            if year and year < filter_config.get("min_year", 0):
                return None

            if mileage and mileage > filter_config.get("max_mileage", float('inf')):
                return None

        return CarCreate(
            title=title,
            link=link,
            price=price,
            brand=filter_config["brand"],
            year=year,
            mileage=mileage,
            features=' | '.join(features) if features else "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö",
            description=description,
            date_posted=date_posted,
            place=place,
            filter_name=filter_config.get("filter_name", "unknown")
        )

    def _scrape_cars_sync(self, filter_config: Dict, existing_links: Set[str]) -> List[CarCreate]:
        """Synchronous scraping —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–æ existing_links"""
        is_urgent = filter_config.get("urgent_mode", False)
        filter_name = filter_config.get("filter_name", "unknown")

        logger.info(f"üåê –ù–∞—á–∏–Ω–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫—Ä–∞–ø–∏–Ω–≥: {filter_name} "
                    f"{'(URGENT —Ä–µ–∂–∏–º)' if is_urgent else '(–æ–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º)'}")
        logger.info(f"üìã –ò—Å–∫–ª—é—á–∞–µ–º {len(existing_links)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Å—ã–ª–æ–∫")

        driver = self._create_driver()
        try:
            driver.get(filter_config["url"])
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located(
                    (By.CSS_SELECTOR, "div.advert.js-item-listing")
                )
            )

            html = driver.page_source
            soup = BeautifulSoup(html, "html.parser")
            debug_file = f"/app/debug_{filter_name}.html"
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(html)
            logger.info(f"üêõ DEBUG: HTML —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {debug_file}")
            # üêõ –ü–†–û–°–¢–û–ô –î–ï–ë–ê–ì
            filter_name = filter_config.get("filter_name", "unknown")
            logger.info(f"üêõ DEBUG {filter_name}: HTML —Ä–∞–∑–º–µ—Ä={len(html)} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"üêõ DEBUG {filter_name}: div —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤—Å–µ–≥–æ={len(soup.find_all('div'))}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –æ–±—ä—è–≤–ª–µ–Ω–∏–π:
            selectors = [
                ("div.advert.js-item-listing", "advert js-item-listing"),
                ("div.advert", "advert"),
                ("[class*='advert']", None),
                (".announcement-item", "announcement-item"),
            ]

            for selector, class_name in selectors:
                if class_name:
                    found = soup.find_all("div", class_=class_name)
                else:
                    found = soup.select(selector)
                logger.info(f"üêõ DEBUG {filter_name}: '{selector}' –Ω–∞–π–¥–µ–Ω–æ {len(found)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∞–≤—Ç–æ:
            all_links = soup.find_all('a', href=True)
            car_links = [a for a in all_links if 'bazaraki.com/adv/' in a.get('href', '')]
            logger.info(f"üêõ DEBUG {filter_name}: –≤—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫={len(all_links)}, –Ω–∞ –∞–≤—Ç–æ={len(car_links)}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã:
            title = soup.find('title')
            page_title = title.text[:100] if title else "–Ω–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞"
            logger.info(f"üêõ DEBUG {filter_name}: –∑–∞–≥–æ–ª–æ–≤–æ–∫='{page_title}'")

            ads = soup.select("div.advert.js-item-listing")
            logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(ads)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –¥–ª—è {filter_name}")

            cars = []
            skipped_existing = 0

            for ad in ads:
                car_data = self._parse_car_data(ad, filter_config, existing_links)
                if car_data:
                    cars.append(car_data)
                else:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏—á–∏–Ω—É –ø—Ä–æ–ø—É—Å–∫–∞
                    should_skip, reason = self._should_skip_ad(ad, existing_links)
                    if reason == "already_exists":
                        skipped_existing += 1

            logger.info(f"‚úÖ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {len(cars)} –ù–û–í–´–• –º–∞—à–∏–Ω –¥–ª—è {filter_name}")
            logger.info(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö: {skipped_existing}")
            logger.info(f"üìä –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {len(cars)} –Ω–æ–≤—ã—Ö / {skipped_existing} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö")

            return cars

        finally:
            driver.quit()

    async def scrape_cars(self, filter_name: str, existing_links: Set[str] = None) -> List[CarCreate]:
        """üéØ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô async –º–µ—Ç–æ–¥ —Å –ø–µ—Ä–µ–¥–∞—á–µ–π existing_links"""
        filter_config = settings.car_filters.get(filter_name)
        if not filter_config:
            logger.warning(f"‚ùå –§–∏–ª—å—Ç—Ä {filter_name} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            return []

        filter_config["filter_name"] = filter_name

        # –ï—Å–ª–∏ existing_links –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π set
        if existing_links is None:
            existing_links = set()
            logger.warning(f"‚ö†Ô∏è existing_links –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω –¥–ª—è {filter_name}, –ø–∞—Ä—Å–∏–º –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è")

        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(None, self._scrape_cars_sync, filter_config, existing_links)

    # üÜï –ù–û–í–´–ï –ú–ï–¢–û–î–´ –î–õ–Ø –û–¢–°–õ–ï–ñ–ò–í–ê–ù–ò–Ø –ò–ó–ú–ï–ù–ï–ù–ò–ô

    async def get_single_car_data(self, car_url: str) -> Optional[Dict[str, Any]]:
        """üéØ –ü–æ–ª—É—á–∞–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Å—ã–ª–∫–µ (–¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
        logger.info(f"üéØ get_single_car_data() called for: {car_url[:50]}...")

        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(None, self._get_single_car_data_sync, car_url)

    def _get_single_car_data_sync(self, car_url: str) -> Optional[Dict[str, Any]]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ–¥–Ω–æ–π –º–∞—à–∏–Ω—ã"""
        driver = self._create_driver()
        try:
            logger.debug(f"üåê Loading page: {car_url}")
            driver.get(car_url)

            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CLASS_NAME, "announcement-block"))
                )
            except:
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ announcement-block, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
                try:
                    WebDriverWait(driver, 5).until(
                        EC.presence_of_element_located((By.CLASS_NAME, "page-content"))
                    )
                except:
                    logger.warning(f"‚ö†Ô∏è Page structure might have changed for: {car_url}")

            html = driver.page_source
            soup = BeautifulSoup(html, "html.parser")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç "–æ–±—ä—è–≤–ª–µ–Ω–∏–µ —É–¥–∞–ª–µ–Ω–æ"
            if self._is_ad_removed(soup):
                logger.info(f"‚ùå Ad removed/unavailable: {car_url}")
                return None

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–µ–Ω—É
            price = self._extract_price_from_page(soup)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
            description = self._extract_description_from_page(soup)

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã
            title = self._extract_title_from_page(soup)

            result = {
                "price": price,
                "description": description,
                "title": title,
                "url": car_url,
                "last_updated": datetime.now().isoformat()
            }

            logger.debug(f"‚úÖ Single car data extracted: price={price}, desc_len={len(description or '')}")
            return result

        except Exception as e:
            logger.error(f"‚ùå Error getting single car data from {car_url}: {e}")
            return None
        finally:
            driver.quit()

    def _is_ad_removed(self, soup: BeautifulSoup) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É–¥–∞–ª–µ–Ω–æ –ª–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ"""
        # –ò—â–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —É–¥–∞–ª–µ–Ω–Ω–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        removed_indicators = [
            "–æ–±—ä—è–≤–ª–µ–Ω–∏–µ —É–¥–∞–ª–µ–Ω–æ",
            "ad has been removed",
            "404",
            "–Ω–µ –Ω–∞–π–¥–µ–Ω–æ",
            "not found",
            "page not found"
        ]

        page_text = soup.get_text().lower()
        return any(indicator in page_text for indicator in removed_indicators)

    def _extract_price_from_page(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–µ–Ω—É —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ü–µ–Ω—ã
        price_selectors = [
            ".announcement-price__cost",
            ".price-section .price",
            ".announcement-block .price",
            "[data-testid='price']",
            ".price-block .price",
            ".cost-primary"
        ]

        for selector in price_selectors:
            price_element = soup.select_one(selector)
            if price_element:
                price_text = price_element.get_text(strip=True)
                if price_text and any(c.isdigit() for c in price_text):
                    logger.debug(f"üí∞ Price found with selector '{selector}': {price_text}")
                    return price_text

        # Fallback: –∏—â–µ–º —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∞—â–∏–π ‚Ç¨ –∏ —Ü–∏—Ñ—Ä—ã
        for element in soup.find_all(text=True):
            if '‚Ç¨' in element and any(c.isdigit() for c in element):
                price_text = element.strip()
                if len(price_text) < 50:  # –†–∞–∑—É–º–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è —Ü–µ–Ω—ã
                    logger.debug(f"üí∞ Price found via fallback: {price_text}")
                    return price_text

        logger.warning("üí∞ Price not found on page")
        return ""

    def _extract_description_from_page(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è
        description_selectors = [
            ".js-description",
            ".announcement-description",
            ".description-text",
            "[data-testid='description']",
            ".announcement-block .description",
            ".ad-description"
        ]

        for selector in description_selectors:
            desc_element = soup.select_one(selector)
            if desc_element:
                # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –≤—Å–µ—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
                paragraphs = desc_element.find_all(['p', 'div', 'span'])
                if paragraphs:
                    desc_text = ' '.join(p.get_text(' ', strip=True) for p in paragraphs)
                else:
                    desc_text = desc_element.get_text(' ', strip=True)

                if desc_text and len(desc_text.strip()) > 10:
                    logger.debug(f"üìù Description found with selector '{selector}': {len(desc_text)} chars")
                    return desc_text.strip()

        logger.warning("üìù Description not found on page")
        return ""

    def _extract_title_from_page(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
        title_selectors = [
            ".announcement-title",
            ".page-title h1",
            ".ad-title",
            "h1.title",
            "[data-testid='title']"
        ]

        for selector in title_selectors:
            title_element = soup.select_one(selector)
            if title_element:
                title_text = title_element.get_text(strip=True)
                if title_text:
                    logger.debug(f"üè∑Ô∏è Title found: {title_text[:50]}")
                    return title_text

        # Fallback: title –∏–∑ <title> —Ç–µ–≥–∞
        title_tag = soup.find('title')
        if title_tag:
            title_text = title_tag.get_text(strip=True)
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —á–∞—Å—Ç–∏ —Ç–∏–ø–∞ "| Bazaraki"
            if '|' in title_text:
                title_text = title_text.split('|')[0].strip()
            return title_text

        return ""

    async def get_available_filters(self) -> Dict[str, Dict]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ä–µ–∂–∏–º–µ"""
        filters_info = {}

        for name, config in settings.car_filters.items():
            filters_info[name] = {
                "brand": config["brand"],
                "min_year": config["min_year"],
                "max_mileage": config["max_mileage"],
                "urgent_mode": config.get("urgent_mode", False),
                "price_range": self._extract_price_range(config["url"])
            }

        return filters_info

    def _extract_price_range(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–µ–Ω–æ–≤–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω –∏–∑ URL"""
        try:
            price_min_match = re.search(r'price_min=(\d+)', url)
            price_max_match = re.search(r'price_max=(\d+)', url)

            if price_min_match and price_max_match:
                min_price = int(price_min_match.group(1))
                max_price = int(price_max_match.group(1))
                return f"‚Ç¨{min_price:,} - ‚Ç¨{max_price:,}"
        except:
            pass
        return "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"


def _debug_page_content(self, soup: BeautifulSoup, filter_name: str):
    """üêõ –î–µ–±–∞–≥ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    logger.info(f"üêõ DEBUG {filter_name} - –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã:")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
    all_divs = soup.find_all("div")
    logger.info(f"üìÑ –í—Å–µ–≥–æ div —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(all_divs)}")

    # –ò—â–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è –ø–æ —Ä–∞–∑–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
    selectors_to_try = [
        "div.advert.js-item-listing",  # –¢–µ–∫—É—â–∏–π
        "div.advert",  # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π
        "div[class*='advert']",  # –õ—é–±–æ–π —Å advert –≤ –∫–ª–∞—Å—Å–µ
        "div[class*='item']",  # –õ—é–±–æ–π —Å item –≤ –∫–ª–∞—Å—Å–µ
        "div[class*='listing']",  # –õ—é–±–æ–π —Å listing –≤ –∫–ª–∞—Å—Å–µ
        ".announcement-item",  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π
        ".ad-item",  # –ï—â–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π
        "[data-testid*='ad']",  # –ü–æ data-testid
    ]

    for selector in selectors_to_try:
        found = soup.select(selector)
        logger.info(f"üîç '{selector}': –Ω–∞–π–¥–µ–Ω–æ {len(found)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")

        if len(found) > 0 and len(found) <= 5:
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–µ—Ä–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            for i, elem in enumerate(found[:2]):
                classes = elem.get('class', [])
                logger.info(f"  –≠–ª–µ–º–µ–Ω—Ç {i + 1}: –∫–ª–∞—Å—Å—ã={classes}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–∞—à–∏–Ω—ã
    car_links = soup.find_all("a", href=True)
    bazaraki_links = [a for a in car_links if "bazaraki.com/adv/" in a.get('href', '')]
    logger.info(f"üîó –°—Å—ã–ª–æ–∫ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {len(bazaraki_links)}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    title = soup.find("title")
    page_title = title.text if title else "–ù–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞"
    logger.info(f"üìÑ –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_title}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –æ—à–∏–±–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    error_indicators = [
        "404", "not found", "page not found",
        "error", "–æ—à–∏–±–∫–∞", "–Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤", "no results"
    ]
    page_text = soup.get_text().lower()
    for indicator in error_indicators:
        if indicator in page_text:
            logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–±–ª–µ–º—ã: '{indicator}'")