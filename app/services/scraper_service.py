# app/services/scraper_service.py - –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Å—ã–ª–æ–∫
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import re
import asyncio
from typing import List, Dict, Optional, Set
from app.config import settings
from app.schemas.car import CarCreate
import httpx
import logging

logger = logging.getLogger(__name__)


class ScraperService:
    def __init__(self):
        self.options = Options()
        self.options.add_argument('--headless')
        self.options.add_argument('--disable-gpu')
        self.options.add_argument('--no-sandbox')
        self.options.add_argument('--disable-dev-shm-usage')

    def _fetch_description(self, link: str) -> Optional[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ"""
        try:
            with httpx.Client(timeout=10.0) as client:
                resp = client.get(link)
                if resp.status_code != 200:
                    return None
                soup = BeautifulSoup(resp.text, 'html.parser')
                desc_div = soup.find('div', class_='js-description')
                if desc_div:
                    paragraphs = [p.get_text(' ', strip=True) for p in desc_div.find_all('p')]
                    return ' '.join(paragraphs)
        except Exception:
            return None
        return None

    def _create_driver(self) -> webdriver.Chrome:
        """Create Chrome driver using path from settings"""
        service = Service(executable_path=settings.chromedriver_path)
        return webdriver.Chrome(service=service, options=self.options)

    def _has_urgent_keywords(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ urgent –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        if not text:
            return False

        urgent_keywords = [
            '—Å—Ä–æ—á–Ω–æ', 'urgent', '–±—ã—Å—Ç—Ä–æ', 'asap', 'must sell', 'price drop',
            'reduced', 'negotiable', 'open to offers', '—Ç–æ—Ä–≥', '–æ–±–º–µ–Ω',
            '–≤—ã–≥–æ–¥–Ω–æ', '–Ω–µ–¥–æ—Ä–æ–≥–æ', '–¥–µ—à–µ–≤–æ', '—Å–Ω–∏–∂–µ–Ω–∞ —Ü–µ–Ω–∞', '—É–º–µ—Å—Ç–µ–Ω —Ç–æ—Ä–≥'
        ]

        text_lower = text.lower()
        return any(keyword in text_lower for keyword in urgent_keywords)

    def _should_skip_ad(self, ad, existing_links: Set[str]) -> tuple[bool, str]:
        """üéØ –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏–µ"""
        title_tag = ad.find("a", class_="advert__content-title")
        if not title_tag:
            return True, "no_title_tag"

        link = "https://www.bazaraki.com" + title_tag.get('href', '')

        if link in existing_links:
            return True, "already_exists"

        return False, ""

    def _parse_car_data(self, ad, filter_config: Dict, existing_links: Set[str]) -> Optional[CarCreate]:
        """–ü–∞—Ä—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ –∞–≤—Ç–æ–º–æ–±–∏–ª—è —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è"""

        # –°–Ω–∞—á–∞–ª–∞ –±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –µ—Å—Ç—å –ª–∏ —É–∂–µ –≤ –±–∞–∑–µ
        should_skip, reason = self._should_skip_ad(ad, existing_links)
        if should_skip:
            if reason == "already_exists":
                logger.debug("‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ")
            return None

        # Title –∏ link (—É–∂–µ –ø—Ä–æ–≤–µ—Ä–∏–ª–∏ –≤ _should_skip_ad)
        title_tag = ad.find("a", class_="advert__content-title")
        title = title_tag.text.strip()
        link = "https://www.bazaraki.com" + title_tag.get('href', '')

        # Price
        price_tag = ad.find("a", class_="advert__content-price")
        price = price_tag.text.strip().replace("\n", " ") if price_tag else "–Ω–µ—Ç —Ü–µ–Ω—ã"

        # Parse features
        features_tag = ad.find("div", class_="advert__content-features")
        features = []
        mileage = None
        year = None

        if features_tag:
            for f in features_tag.find_all("div", class_="advert__content-feature"):
                text = f.text.strip()
                features.append(text)

                # Parse mileage
                mileage_match = re.search(r'([\d,. ]+)\s*km', text.lower())
                if mileage_match:
                    mileage_str = mileage_match.group(1).replace(',', '').replace(' ', '')
                    try:
                        mileage = int(mileage_str)
                    except:
                        pass

                # Parse year
                year_match = re.search(r'(19\d{2}|20\d{2})', text)
                if year_match:
                    try:
                        year = int(year_match.group(1))
                    except:
                        pass

        # Parse year from title if not found
        if year is None:
            title_year_match = re.search(r'(19\d{2}|20\d{2})', title)
            if title_year_match:
                try:
                    year = int(title_year_match.group(1))
                except:
                    pass

        # Date and place
        hint_tag = ad.find("div", class_="advert__content-hint")
        date_posted, place = "–Ω–µ—Ç –¥–∞—Ç—ã", "–Ω–µ—Ç –≥–æ—Ä–æ–¥–∞"

        if hint_tag:
            date_tag = hint_tag.find("div", class_="advert__content-date")
            if date_tag:
                date_posted = date_tag.text.strip()

            place_tag = hint_tag.find("div", class_="advert__content-place")
            if place_tag:
                place = place_tag.text.strip()

        # Description (–∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ - —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π!)
        desc_tag = ad.find("div", class_="advert__description")
        if not desc_tag:
            desc_tag = ad.find("div", class_="advert__content-description")
        description = desc_tag.text.strip() if desc_tag else None
        if not description:
            description = self._fetch_description(link)

        # üî• URGENT MODE LOGIC - –±–æ–ª–µ–µ –º—è–≥–∫–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã
        is_urgent_mode = filter_config.get("urgent_mode", False)

        if is_urgent_mode:
            # –í urgent —Ä–µ–∂–∏–º–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            has_urgent_text = self._has_urgent_keywords(title) or self._has_urgent_keywords(description)

            # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–ª—è urgent
            if year and year < filter_config.get("min_year", 0):
                if year < (filter_config.get("min_year", 0) - 2):
                    return None

            if mileage and mileage > filter_config.get("max_mileage", float('inf')):
                max_allowed = filter_config.get("max_mileage", float('inf'))
                if has_urgent_text:
                    max_allowed += 50000  # –ë–æ–Ω—É—Å –¥–ª—è urgent –æ–±—ä—è–≤–ª–µ–Ω–∏–π
                if mileage > max_allowed:
                    return None

            logger.info(f"üî• Urgent —Ä–µ–∂–∏–º: {filter_config.get('filter_name')} - –Ω–∞–π–¥–µ–Ω–∞ –º–∞—à–∏–Ω–∞"
                        f" {'(URGENT keywords!)' if has_urgent_text else ''}")
        else:
            # –û–±—ã—á–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            if year and year < filter_config.get("min_year", 0):
                return None

            if mileage and mileage > filter_config.get("max_mileage", float('inf')):
                return None

        return CarCreate(
            title=title,
            link=link,
            price=price,
            brand=filter_config["brand"],
            year=year,
            mileage=mileage,
            features=' | '.join(features) if features else "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö",
            description=description,
            date_posted=date_posted,
            place=place,
            filter_name=filter_config.get("filter_name", "unknown")
        )

    def _scrape_cars_sync(self, filter_config: Dict, existing_links: Set[str]) -> List[CarCreate]:
        """Synchronous scraping —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–æ existing_links"""
        is_urgent = filter_config.get("urgent_mode", False)
        filter_name = filter_config.get("filter_name", "unknown")

        logger.info(f"üåê –ù–∞—á–∏–Ω–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫—Ä–∞–ø–∏–Ω–≥: {filter_name} "
                    f"{'(URGENT —Ä–µ–∂–∏–º)' if is_urgent else '(–æ–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º)'}")
        logger.info(f"üìã –ò—Å–∫–ª—é—á–∞–µ–º {len(existing_links)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Å—ã–ª–æ–∫")

        driver = self._create_driver()
        try:
            driver.get(filter_config["url"])
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located(
                    (By.CSS_SELECTOR, "div.advert.js-item-listing")
                )
            )

            html = driver.page_source
            soup = BeautifulSoup(html, "html.parser")
            ads = soup.find_all("div", class_="advert js-item-listing")

            logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(ads)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –¥–ª—è {filter_name}")

            cars = []
            skipped_existing = 0

            for ad in ads:
                car_data = self._parse_car_data(ad, filter_config, existing_links)
                if car_data:
                    cars.append(car_data)
                else:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏—á–∏–Ω—É –ø—Ä–æ–ø—É—Å–∫–∞
                    should_skip, reason = self._should_skip_ad(ad, existing_links)
                    if reason == "already_exists":
                        skipped_existing += 1

            logger.info(f"‚úÖ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {len(cars)} –ù–û–í–´–• –º–∞—à–∏–Ω –¥–ª—è {filter_name}")
            logger.info(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö: {skipped_existing}")
            logger.info(f"üìä –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {len(cars)} –Ω–æ–≤—ã—Ö / {skipped_existing} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö")

            return cars

        finally:
            driver.quit()

    async def scrape_cars(self, filter_name: str, existing_links: Set[str] = None) -> List[CarCreate]:
        """üéØ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô async –º–µ—Ç–æ–¥ —Å –ø–µ—Ä–µ–¥–∞—á–µ–π existing_links"""
        filter_config = settings.car_filters.get(filter_name)
        if not filter_config:
            logger.warning(f"‚ùå –§–∏–ª—å—Ç—Ä {filter_name} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            return []

        filter_config["filter_name"] = filter_name

        # –ï—Å–ª–∏ existing_links –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π set
        if existing_links is None:
            existing_links = set()
            logger.warning(f"‚ö†Ô∏è existing_links –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω –¥–ª—è {filter_name}, –ø–∞—Ä—Å–∏–º –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è")

        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(None, self._scrape_cars_sync, filter_config, existing_links)

    async def get_available_filters(self) -> Dict[str, Dict]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ä–µ–∂–∏–º–µ"""
        filters_info = {}

        for name, config in settings.car_filters.items():
            filters_info[name] = {
                "brand": config["brand"],
                "min_year": config["min_year"],
                "max_mileage": config["max_mileage"],
                "urgent_mode": config.get("urgent_mode", False),
                "price_range": self._extract_price_range(config["url"])
            }

        return filters_info

    def _extract_price_range(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–µ–Ω–æ–≤–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω –∏–∑ URL"""
        try:
            price_min_match = re.search(r'price_min=(\d+)', url)
            price_max_match = re.search(r'price_max=(\d+)', url)

            if price_min_match and price_max_match:
                min_price = int(price_min_match.group(1))
                max_price = int(price_max_match.group(1))
                return f"‚Ç¨{min_price:,} - ‚Ç¨{max_price:,}"
        except:
            pass
        return "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"